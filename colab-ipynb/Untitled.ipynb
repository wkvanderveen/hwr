{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "InaNmg-wkzTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive/\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHRh6_5c0VEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip3 install wget"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnnQ1f_J4EEk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!rm -rf \"gdrive/My Drive/HWR/data/letters-train/\"\n",
        "#!rm -rf \"gdrive/My Drive/HWR/data/letters-test/\"\n",
        "#!rm -rf \"gdrive/My Drive/HWR/data/lines-train/\"\n",
        "#!rm -rf \"gdrive/My Drive/HWR/data/lines-test/\"\n",
        "!rm -rf \"gdrive/My Drive/HWR/data/checkpoint/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlTLiI4doWiu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os, sys\n",
        "from os.path import abspath\n",
        "sys.path.append(abspath(\"gdrive/My Drive/HWR/src\"))\n",
        "from time import sleep\n",
        "from split import Splitter\n",
        "from data_augmenter import Augmenter\n",
        "from construct_train_images import Linemaker\n",
        "from math import ceil\n",
        "from make_tfrecords import TfRecordMaker\n",
        "from kmeans import AnchorMaker\n",
        "from convert_weight import WeightConverter\n",
        "from quick_train import Trainer\n",
        "from show_input_image import ExampleDisplayer\n",
        "from quick_test import Tester\n",
        "\n",
        "## PARAMETERS ##\n",
        "\n",
        "# File structure parameters\n",
        "orig_letters_dir = \"gdrive/My Drive/HWR/data/original_letters/\"\n",
        "letters_train_dir = \"gdrive/My Drive/HWR/data/letters-train/\"\n",
        "letters_test_dir = \"gdrive/My Drive/HWR/data/letters-test/\"\n",
        "lines_train_dir = \"gdrive/My Drive/HWR/data/lines-train/\"\n",
        "lines_test_dir = \"gdrive/My Drive/HWR/data/lines-test/\"\n",
        "label_train_path = \"gdrive/My Drive/HWR/data/labels-train.txt\"\n",
        "label_test_dir = \"gdrive/My Drive/HWR/data/labels-test.txt\"\n",
        "checkpoint_dir = \"gdrive/My Drive/HWR/data/checkpoint/\"\n",
        "dimensions_file = \"gdrive/My Drive/HWR/data/dimensions.txt\"\n",
        "weights_dir = \"gdrive/My Drive/HWR/data/weights/\"\n",
        "anchor_file = \"gdrive/My Drive/HWR/data/anchors.txt\"\n",
        "\n",
        "# Data parameters\n",
        "num_classes = 2  # not too few\n",
        "split_percentage = 20\n",
        "line_length_bounds = (20,50)\n",
        "n_training_lines = 50\n",
        "n_testing_lines = 50\n",
        "max_overlap_train = 20\n",
        "max_overlap_test = 20\n",
        "\n",
        "# Network parameters\n",
        "cluster_num = 9\n",
        "iou_threshold = 0.1\n",
        "score_threshold = 0.1\n",
        "batch_size = 8\n",
        "steps = 100\n",
        "learning_rate = 1e-3\n",
        "decay_steps = 100\n",
        "decay_rate = 0.9\n",
        "shuffle_size = 200\n",
        "eval_internal = 10\n",
        "save_internal = 50\n",
        "\n",
        "# Other parameters\n",
        "retrain = False\n",
        "show_tfrecord_example = True\n",
        "test_example = False\n",
        "evaluate_network = False # TBA\n",
        "\n",
        "\n",
        "# READ INPUT\n",
        "\n",
        "# BINARIZE INPUT\n",
        "\n",
        "# SEGMENT LINES FROM INPUT\n",
        "\n",
        "\n",
        "# PREPARE NETWORK IF NOT READY\n",
        "network_exists = bool(os.path.isfile(\"../../data/checkpoint/checkpoint\"))\n",
        "\n",
        "if not network_exists or retrain:\n",
        "\n",
        "    if not os.path.exists(letters_train_dir):\n",
        "        splitter = Splitter(source_dir=orig_letters_dir,\n",
        "                            num_classes=num_classes,\n",
        "                            train_dir=letters_train_dir,\n",
        "                            test_dir=letters_test_dir,\n",
        "                            percentage=split_percentage)\n",
        "        print(f\"Splitting {splitter.percentage}% of the data found in {splitter.source_dir}...\")\n",
        "        sleep(1)\n",
        "        num_classes = splitter.split()\n",
        "\n",
        "        print(f\"Augmenting training letters...\")\n",
        "        augmenter = Augmenter(source_dir=letters_train_dir, shear=True, coarse_dropout=(0.02, 0.5))\n",
        "        augmenter.augment()\n",
        "    else:\n",
        "        print(\"Training dataset detected! Skipping splitting & augmenting.\")\n",
        "        sleep(1)\n",
        "        num_classes = len(os.listdir(letters_train_dir))\n",
        "\n",
        "\n",
        "    if not os.path.exists(lines_train_dir):\n",
        "        linemaker = Linemaker(set_type=\"train\",\n",
        "                              source_dir=letters_train_dir,\n",
        "                              target_dir=lines_train_dir,\n",
        "                              label_dir=label_train_path,\n",
        "                              line_length_bounds=line_length_bounds,\n",
        "                              n_lines=n_training_lines,\n",
        "                              max_overlap=max_overlap_train)\n",
        "        max_h1, max_w1 = linemaker.make_lines()\n",
        "\n",
        "        linemaker = Linemaker(set_type=\"test\",\n",
        "                              source_dir=letters_test_dir,\n",
        "                              target_dir=lines_test_dir,\n",
        "                              label_dir=label_test_dir,\n",
        "                              line_length_bounds=line_length_bounds,\n",
        "                              n_lines=n_testing_lines,\n",
        "                              max_overlap=max_overlap_test)\n",
        "        max_h2, max_w2 = linemaker.make_lines()\n",
        "\n",
        "        max_h = ceil(max(max_h1, max_h2)/32.0)*32\n",
        "        max_w = ceil(max(max_w1, max_w2)/32.0)*32\n",
        "        img_dims = (max_h, max_w)\n",
        "\n",
        "        with open(dimensions_file, \"w+\") as filename:\n",
        "            print(f\"{max_h} {max_w}\", file=filename)\n",
        "        img_dims = (max_h, max_w)\n",
        "\n",
        "\n",
        "    else:\n",
        "        print(\"Line data detected! Skipping linemaking.\")\n",
        "        sleep(1)\n",
        "\n",
        "        with open(dimensions_file, \"r\") as max_dimensions:\n",
        "            img_h, img_w = [int(x) for x in max_dimensions.read().split()]\n",
        "        img_dims = (img_h, img_w)\n",
        "\n",
        "\n",
        "    if not os.path.isfile(os.path.normpath(lines_train_dir) + \".tfrecords\"):\n",
        "        print(\"Making tfrecords...\")\n",
        "        recordmaker = TfRecordMaker(imgs_dir=lines_train_dir, label_path=label_train_path, colab=True)\n",
        "        recordmaker.make_records()\n",
        "        recordmaker = TfRecordMaker(imgs_dir=lines_test_dir, label_path=label_test_dir, colab=True)\n",
        "        recordmaker.make_records()\n",
        "    else:\n",
        "        print(\"Not creating TfRecords files because they already exist!\")\n",
        "        sleep(1)\n",
        "\n",
        "    if not os.path.isfile(anchor_file):\n",
        "        print(\"Making anchors...\")\n",
        "        anchormaker = AnchorMaker(target_file=anchor_file,\n",
        "                                  label_path=label_train_path,\n",
        "                                  cluster_num=cluster_num,\n",
        "                                  colab=True)\n",
        "        anchormaker.make_anchors()\n",
        "\n",
        "    else:\n",
        "        print(\"Not creating anchors file because it already exists!\")\n",
        "        sleep(1)\n",
        "\n",
        "    if not os.path.exists(weights_dir):\n",
        "        print(\"Error: no weights detected! You need the pretrained \" +\n",
        "              f\"weights in the {weights_dir} directory.\")\n",
        "\n",
        "    weightconverter = WeightConverter(freeze=False,\n",
        "                                      convert=True,\n",
        "                                      num_classes=num_classes,\n",
        "                                      img_dims=img_dims,\n",
        "                                      checkpoint_dir=checkpoint_dir,\n",
        "                                      weights_dir=weights_dir,\n",
        "                                      anchors_path=anchor_file,\n",
        "                                      score_threshold=score_threshold,\n",
        "                                      iou_threshold=iou_threshold)\n",
        "    print(\"Converting weights...\")\n",
        "    weightconverter.convert_weights()\n",
        "\n",
        "    trainer = Trainer(num_classes=num_classes,\n",
        "                      batch_size=batch_size,\n",
        "                      steps=steps,\n",
        "                      learning_rate=learning_rate,\n",
        "                      decay_steps=decay_steps,\n",
        "                      decay_rate=decay_rate,\n",
        "                      shuffle_size=shuffle_size,\n",
        "                      eval_internal=eval_internal,\n",
        "                      save_internal=save_internal,\n",
        "                      img_dims=img_dims,\n",
        "                      anchors_path=anchor_file,\n",
        "                      train_records=os.path.normpath(lines_train_dir) + \".tfrecords\",\n",
        "                      test_records=os.path.normpath(lines_test_dir) + \".tfrecords\",\n",
        "                      checkpoint_path=checkpoint_dir)\n",
        "    print(\"Training network...\")\n",
        "    trainer.train()\n",
        "\n",
        "    network_exists = True\n",
        "\n",
        "else: # if network already exists and not retraining\n",
        "    print(\"Network already trained!\")\n",
        "    sleep(1)\n",
        "\n",
        "    with open(dimensions_file, \"r\") as max_dimensions:\n",
        "        img_h, img_w = [int(x) for x in max_dimensions.read().split()]\n",
        "    img_dims = (img_h, img_w)\n",
        "    num_classes = len(os.listdir(letters_train_dir))\n",
        "\n",
        "\n",
        "if network_exists and show_tfrecord_example:\n",
        "    example_displayer = ExampleDisplayer(source_dir=os.path.normpath(lines_train_dir) + \".tfrecords\",\n",
        "                                         img_dims=img_dims,\n",
        "                                         anchor_dir=anchor_file,\n",
        "                                         num_classes=num_classes)\n",
        "    example_displayer.show_example()\n",
        "\n",
        "if network_exists and test_example:\n",
        "\n",
        "    weightconverter = WeightConverter(freeze=True,\n",
        "                                      num_classes=num_classes,\n",
        "                                      img_dims=img_dims,\n",
        "                                      checkpoint_dir=checkpoint_dir,\n",
        "                                      weights_dir=weights_dir,\n",
        "                                      anchors_path=anchor_file,\n",
        "                                      score_threshold=score_threshold,\n",
        "                                      iou_threshold=iou_threshold,\n",
        "                                      convert=False,\n",
        "                                      checkpoint_step=steps-(steps%save_internal))\n",
        "    weightconverter.convert_weights()\n",
        "\n",
        "    tester = Tester(source_dir=lines_test_dir,\n",
        "                    num_classes=num_classes,\n",
        "                    score_threshold=score_threshold,\n",
        "                    iou_threshold=iou_threshold,\n",
        "                    img_dims=img_dims,\n",
        "                    checkpoint_dir=checkpoint_dir,\n",
        "                    letters_test_dir=letters_test_dir)\n",
        "    tester.test()\n",
        "\n",
        "if network_exists and evaluate_network:\n",
        "    pass\n",
        "    # evaluater = Evaluater()\n",
        "    # evaluater.eval()\n",
        "\n",
        "\n",
        "# FOR LINE IN LINES:\n",
        "    # FEED LINE TO NETWORK, GET RESULT\n",
        "\n",
        "    # POSTPROCESS CHARACTER LIKELIHOODS\n",
        "\n",
        "    # PRINT PREDICTION\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "TODO:\n",
        "* Put data augmentation stuff in parameters here\n",
        "* Actually train on the data to see if YOLO works in the first place\n",
        "\n",
        "\n",
        "\"\"\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}